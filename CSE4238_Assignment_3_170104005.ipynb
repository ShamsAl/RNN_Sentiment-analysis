{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE4238_Assignment-3_170104005.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "nW4ElwLgysH_",
        "Ay2TDwZCy96_",
        "9RQpoQIizIfo",
        "tzq1MJWAzLy3",
        "BnThLglw3HEu"
      ],
      "authorship_tag": "ABX9TyNna33UF5DbGiEaq9j7AsNA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShamsAl/RNN_Sentiment-analysis/blob/main/CSE4238_Assignment_3_170104005.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okAZVx9nb9Es"
      },
      "source": [
        "# Drive mount code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re1cOmeea_7J"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('./drive')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW4ElwLgysH_"
      },
      "source": [
        "# Dataset Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay2TDwZCy96_"
      },
      "source": [
        "## Download dataset 3\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlb4s-FrcGxi",
        "outputId": "6d6b2318-2016-424b-dfc9-a9c07c01b12a"
      },
      "source": [
        "!gdown --id 1hgfcQHIlfnnDTSUAA_3--m-YA7BelKl3"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hgfcQHIlfnnDTSUAA_3--m-YA7BelKl3\n",
            "To: /content/Dataset 3.csv\n",
            "\r  0% 0.00/999k [00:00<?, ?B/s]\r100% 999k/999k [00:00<00:00, 43.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RQpoQIizIfo"
      },
      "source": [
        "## Import package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLl0_bE2cC-C",
        "outputId": "d3798be2-3312-488f-8f1c-5bd83f5184e6"
      },
      "source": [
        "# import some importent library or packages \n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import time,sys\n",
        "import copy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pathlib\n",
        "import zipfile\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import torch.optim as optim\n",
        "import pathlib\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "from sklearn.utils import shuffle\n",
        "!pip install torchviz\n",
        "from torchviz import make_dot, make_dot_from_trace\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.7/dist-packages (0.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.9.0+cu102)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (3.7.4.3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fbf2c0dd5f0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzq1MJWAzLy3"
      },
      "source": [
        "## Read downloaded dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SG4wK48ZcGFy"
      },
      "source": [
        "df = pd.read_csv('/content/Dataset 3.csv',encoding='unicode_escape')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZCvTyl6keZe",
        "outputId": "1c5a9e5d-5bbf-47cc-9a96-90424b5cb81d"
      },
      "source": [
        "df.columns ## columns name"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['tweets', 'sentiment'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8lCKytvnQxF",
        "outputId": "0db4ec76-333a-431c-96ef-afd6af0df34a"
      },
      "source": [
        "df['sentiment'].value_counts() ## get number of data in `polarity` class"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    8000\n",
              "1    2314\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU8m4CcPkiV1",
        "outputId": "f268191d-b9f9-4764-c7e4-68cae3ac2dbe"
      },
      "source": [
        "df['tweets'].value_counts()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Mom's depression tied to kids' emotional, intellectual development  https://ift.tt/2HtVZCEÃÂ                                                   11\n",
              "25-04-18                                                                                                                                         4\n",
              "More than 1 in 20 US children and teens have anxiety or depression  https://www.sciencedaily.com/releases/2018/04/180424184119.htmÃÂ Ã¢ÂÂ¦     4\n",
              "[Zicutake] Depression can be prevented by exercise  https://goo.gl/fb/Mu7k6tÃÂ                                                                  3\n",
              "suffering from chronic depression https://vine.co/v/iu3l2lPBhiKÃÂ                                                                               3\n",
              "                                                                                                                                                ..\n",
              "@mojustice I've heard this fall. I'm waiting too!                                                                                                1\n",
              "Going to say my prayers and go to sleep!                                                                                                         1\n",
              "ItÃ¢â¬â¢s always a good idea to match your umbrella w/your outfit  http://bit.ly/tgBM9                                                         1\n",
              "still feeling terrible and sick but hooray for Megan Hilty (aka one of the most hilarious Glindas ever) on desperate Housewives tonight          1\n",
              "@GADataGuy You too have a great day                                                                                                              1\n",
              "Name: tweets, Length: 10282, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnThLglw3HEu"
      },
      "source": [
        "## Dataset splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NddseGZ0n-Y8"
      },
      "source": [
        "## split dataset based on the class\n",
        "traning_split_size = 0.8\n",
        "df_class_1 = df[df['sentiment'] == 1]\n",
        "df_class_0 = df[df['sentiment'] == 0]\n",
        "\n",
        "trainSize = int(len(df_class_0) * traning_split_size)\n",
        "Traning_class_0 = df_class_0[:trainSize]\n",
        "Test_class_0 = df_class_0[trainSize:]\n",
        "\n",
        "trainSize = int(len(df_class_1) * traning_split_size)\n",
        "Traning_class_1 = df_class_1[:trainSize]\n",
        "Test_class_1 = df_class_1[trainSize:]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MJnpde_qvabR",
        "outputId": "f24a8e5b-b646-4fbd-bb1e-d9dd0e174dfa"
      },
      "source": [
        "## traning dataset create\n",
        "li = [Traning_class_0,Traning_class_1]\n",
        "frame = pd.concat(li, axis=0, ignore_index=True)\n",
        "frame = shuffle(frame)\n",
        "frame.reset_index(inplace=True, drop=True)\n",
        "frame.to_csv('Train-Dataset.csv',index=False)\n",
        "frame"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@ReneeCarrollAZ @KyleKashuv These snowflakes w...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>loving summer</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>About to pass out! Super tired...good night tw...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Photo: havent-got-a-prayer: Me too  http://tum...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@macitout and recording music here with Nona H...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8246</th>\n",
              "      <td>#GeneChat Large-population studies that look f...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8247</th>\n",
              "      <td>I spent all of 10th grade crying for absolutel...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8248</th>\n",
              "      <td>I mean I think I'm just being funny but appare...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8249</th>\n",
              "      <td>@comeagainjen Jenn, I just wanted to say that ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8250</th>\n",
              "      <td>@Lkmcn1 blimey - predictive text is a bugger, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8251 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 tweets  sentiment\n",
              "0     @ReneeCarrollAZ @KyleKashuv These snowflakes w...          1\n",
              "1                                        loving summer           0\n",
              "2     About to pass out! Super tired...good night tw...          0\n",
              "3     Photo: havent-got-a-prayer: Me too  http://tum...          0\n",
              "4     @macitout and recording music here with Nona H...          0\n",
              "...                                                 ...        ...\n",
              "8246  #GeneChat Large-population studies that look f...          1\n",
              "8247  I spent all of 10th grade crying for absolutel...          1\n",
              "8248  I mean I think I'm just being funny but appare...          1\n",
              "8249  @comeagainjen Jenn, I just wanted to say that ...          0\n",
              "8250  @Lkmcn1 blimey - predictive text is a bugger, ...          0\n",
              "\n",
              "[8251 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UC7KHE3Jxnax",
        "outputId": "00efe2eb-e8dd-4348-e7e7-caf90c0c4380"
      },
      "source": [
        "## testing dataset create\n",
        "li = [Test_class_0,Test_class_1]\n",
        "frame = pd.concat(li, axis=0, ignore_index=True)\n",
        "frame = shuffle(frame)\n",
        "frame.reset_index(inplace=True, drop=True)\n",
        "frame.to_csv('Test-Dataset.csv',index=False)\n",
        "frame"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@rawrAlii   Looking for Alice......nice to mee...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Just just stop! Stop doingg things that you mi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>@wowie It's a good movie, I like that one. Nev...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lowpass on pizzicato strings - number one caus...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@Rotmush no.. I think it was Ori</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2058</th>\n",
              "      <td>@amerj thanks</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2059</th>\n",
              "      <td>You're the P. Swayer to my B. Davis</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2060</th>\n",
              "      <td>@aaronmcarroll i try my best to inspire you wi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2061</th>\n",
              "      <td>@sjrozas I can pick them up for you. You'll be...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2062</th>\n",
              "      <td>mama mia, here I go again, my my how can i res...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2063 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 tweets  sentiment\n",
              "0     @rawrAlii   Looking for Alice......nice to mee...          0\n",
              "1     Just just stop! Stop doingg things that you mi...          1\n",
              "2     @wowie It's a good movie, I like that one. Nev...          0\n",
              "3     lowpass on pizzicato strings - number one caus...          1\n",
              "4                     @Rotmush no.. I think it was Ori           0\n",
              "...                                                 ...        ...\n",
              "2058                                     @amerj thanks           0\n",
              "2059               You're the P. Swayer to my B. Davis           0\n",
              "2060  @aaronmcarroll i try my best to inspire you wi...          0\n",
              "2061  @sjrozas I can pick them up for you. You'll be...          0\n",
              "2062  mama mia, here I go again, my my how can i res...          0\n",
              "\n",
              "[2063 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JCI-V_ASE-5"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwhD-3_x3t5V"
      },
      "source": [
        "# Text cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vf7x0ff83u0F",
        "outputId": "b35b7868-d463-4c17-8cbd-bba91376108c"
      },
      "source": [
        "# import some importent library or packages \n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import time,sys,re,string\n",
        "import copy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pathlib\n",
        "import zipfile\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "\n",
        "import torch.optim as optim\n",
        "import pathlib\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "!pip install torchsummary\n",
        "from torchsummary import summary\n",
        "from sklearn.utils import shuffle\n",
        "!pip install torchviz\n",
        "from torchviz import make_dot, make_dot_from_trace\n",
        "try:\n",
        "  import contractions\n",
        "except:\n",
        "  !pip install contractions\n",
        "import contractions\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, Vectors, GloVe\n",
        "\n",
        "import nltk\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('averaged_perceptron_tagger')\n",
        "    nltk.data.find('brown')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    nltk.download('brown')\n",
        "    nltk.download('punkt')\n",
        "from nltk import sent_tokenize,word_tokenize\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.7/dist-packages (0.0.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (3.7.4.3)\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fbf2c0dd5f0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqxEVwxP37g2"
      },
      "source": [
        "traning_df = pd.read_csv('/content/Train-Dataset.csv')\n",
        "testing_df = pd.read_csv('/content/Test-Dataset.csv')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "gulKysGivn3d",
        "outputId": "9b8b88e3-e9f1-4e44-d16e-2eb5f81336a8"
      },
      "source": [
        "traning_df"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweets</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@ReneeCarrollAZ @KyleKashuv These snowflakes w...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>loving summer</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>About to pass out! Super tired...good night tw...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Photo: havent-got-a-prayer: Me too  http://tum...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@macitout and recording music here with Nona H...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8246</th>\n",
              "      <td>#GeneChat Large-population studies that look f...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8247</th>\n",
              "      <td>I spent all of 10th grade crying for absolutel...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8248</th>\n",
              "      <td>I mean I think I'm just being funny but appare...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8249</th>\n",
              "      <td>@comeagainjen Jenn, I just wanted to say that ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8250</th>\n",
              "      <td>@Lkmcn1 blimey - predictive text is a bugger, ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8251 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 tweets  sentiment\n",
              "0     @ReneeCarrollAZ @KyleKashuv These snowflakes w...          1\n",
              "1                                        loving summer           0\n",
              "2     About to pass out! Super tired...good night tw...          0\n",
              "3     Photo: havent-got-a-prayer: Me too  http://tum...          0\n",
              "4     @macitout and recording music here with Nona H...          0\n",
              "...                                                 ...        ...\n",
              "8246  #GeneChat Large-population studies that look f...          1\n",
              "8247  I spent all of 10th grade crying for absolutel...          1\n",
              "8248  I mean I think I'm just being funny but appare...          1\n",
              "8249  @comeagainjen Jenn, I just wanted to say that ...          0\n",
              "8250  @Lkmcn1 blimey - predictive text is a bugger, ...          0\n",
              "\n",
              "[8251 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdQ4ajcoOdX6"
      },
      "source": [
        "def clean_text(tweets):\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers. removibng miltiple full stop'''\n",
        "    tweets = str(tweets).lower()\n",
        "    tweets = re.sub('\\[.*?\\]', '', tweets)\n",
        "    tweets = re.sub('https?://\\S+|www\\.\\S+', '', tweets)\n",
        "    tweets = re.sub('<.*?>+', '', tweets)\n",
        "    tweets = re.sub('[%s]' % re.escape(string.punctuation), '', tweets)\n",
        "    tweets = re.sub('\\n', '', tweets)\n",
        "    tweets = re.sub('\\w*\\d\\w*', '', tweets)\n",
        "    tweets = re.sub(r'\\.+', \".\", tweets)\n",
        "    return tweets\n",
        "\n",
        "def replace_text(tweets):\n",
        "    tweets = str(tweets).lower()\n",
        "    tweets = tweets.encode('ascii', 'ignore').decode('utf-8') \n",
        "    return tweets  \n",
        "\n",
        "\n",
        "for dta in [traning_df,testing_df]:\n",
        "  dta['tweets_cleaning'] = dta.tweets.apply(lambda x: x.strip().lower() )\n",
        "  dta['tweets_cleaning'] = dta.tweets_cleaning.apply(lambda x : \" \".join(x.split()) )\n",
        "  dta['tweets_cleaning'] = dta.tweets_cleaning.apply(lambda x: contractions.fix(x) )\n",
        "  dta['tweets_cleaning'] = dta.tweets_cleaning.apply(lambda x: clean_text(x) )\n",
        "  dta['tweets_cleaning'] = dta.tweets_cleaning.apply(lambda x: replace_text(x) )\n",
        "  dta['number_of_word'] = dta.tweets_cleaning.apply(lambda x: len(word_tokenize(x)) )\n",
        "  dta['number_of_letter'] = dta.tweets_cleaning.apply(lambda x: len(x) )\n",
        "  del dta['tweets']\n",
        "  dta['tweets'] = dta['tweets_cleaning'] \n",
        "  del dta['tweets_cleaning']\n",
        "\n",
        "\n",
        "\n",
        "for dta in [traning_df,testing_df]:\n",
        "  word_count_zero = dta[dta['number_of_word'] == 0]\n",
        "  dta.drop(word_count_zero.index,inplace=True)\n",
        "  letter_count_zero = dta[dta['number_of_letter'] == 0]\n",
        "  dta.drop(letter_count_zero.index,inplace=True)\n",
        "\n",
        "\n",
        "# # Training DF\n",
        "# traning_df['text_cleaning'] = traning_df.text.apply(lambda x: x.strip().lower() )\n",
        "# traning_df['text_cleaning'] = traning_df.text_cleaning.apply(lambda x : \" \".join(x.split()) )\n",
        "# traning_df['text_cleaning'] = traning_df.text_cleaning.apply(lambda x: contractions.fix(x) )\n",
        "# traning_df['text_cleaning'] = traning_df.text_cleaning.apply(lambda x: clean_text(x) )\n",
        "# traning_df['text_cleaning'] = traning_df.text_cleaning.apply(lambda x: replace_text(x) )\n",
        "\n",
        "# # Testing DF\n",
        "# testing_df['text_cleaning'] = testing_df.text.apply(lambda x: x.strip().lower() )\n",
        "# testing_df['text_cleaning'] = traning_df.text_cleaning.apply(lambda x : \" \".join(x.split()) )\n",
        "# testing_df['text_cleaning'] = testing_df.text_cleaning.apply(lambda x: contractions.fix(x) )\n",
        "# testing_df['text_cleaning'] = testing_df.text_cleaning.apply(lambda x: clean_text(x) )\n",
        "# testing_df['text_cleaning'] = testing_df.text_cleaning.apply(lambda x: replace_text(x) )\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "OptnUdYqPe_D",
        "outputId": "f8b1295d-be6c-4927-ef28-1d2ad8789a62"
      },
      "source": [
        "traning_df"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>number_of_word</th>\n",
              "      <th>number_of_letter</th>\n",
              "      <th>tweets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>119</td>\n",
              "      <td>reneecarrollaz kylekashuv these snowflakes wou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>loving summer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>64</td>\n",
              "      <td>about to pass out super tiredgood night twitte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>32</td>\n",
              "      <td>photo have notgotaprayer me too</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>118</td>\n",
              "      <td>macitout and recording music here with nona he...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8246</th>\n",
              "      <td>1</td>\n",
              "      <td>45</td>\n",
              "      <td>266</td>\n",
              "      <td>genechat largepopulation studies that look for...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8247</th>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>166</td>\n",
              "      <td>i spent all of  grade crying for absolutely no...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8248</th>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>182</td>\n",
              "      <td>i mean i think i am just being funny but appar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8249</th>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>103</td>\n",
              "      <td>comeagainjen jenn i just wanted to say that i ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8250</th>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>46</td>\n",
              "      <td>blimey  predictive text is a bugger is not it</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8244 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      sentiment  ...                                             tweets\n",
              "0             1  ...  reneecarrollaz kylekashuv these snowflakes wou...\n",
              "1             0  ...                                      loving summer\n",
              "2             0  ...  about to pass out super tiredgood night twitte...\n",
              "3             0  ...                   photo have notgotaprayer me too \n",
              "4             0  ...  macitout and recording music here with nona he...\n",
              "...         ...  ...                                                ...\n",
              "8246          1  ...  genechat largepopulation studies that look for...\n",
              "8247          1  ...  i spent all of  grade crying for absolutely no...\n",
              "8248          1  ...  i mean i think i am just being funny but appar...\n",
              "8249          0  ...  comeagainjen jenn i just wanted to say that i ...\n",
              "8250          0  ...      blimey  predictive text is a bugger is not it\n",
              "\n",
              "[8244 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnDFX37x7tcc",
        "outputId": "53354222-a936-416a-9bb7-5219d8b043e6"
      },
      "source": [
        "alltext_length = []\n",
        "allword_length = []\n",
        "for val in traning_df.tweets:\n",
        "  word_tok = word_tokenize(val)\n",
        "  alltext_length.append(len(val))\n",
        "  allword_length.append(len(word_tok))\n",
        "\n",
        "print(max(alltext_length))\n",
        "print(max(allword_length))\n",
        "\n",
        "# traning_df.text.apply(lambda x: len(x) )"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "289\n",
            "62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doo23zv2ZNj4",
        "outputId": "ccc4a0ab-dc28-427e-b197-ca8dfca0a1a3"
      },
      "source": [
        "for val in traning_df.tweets:\n",
        "  word_tok = word_tokenize(val)\n",
        "  if len(word_tok) <= 1:\n",
        "    print(val)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "twittermaniaaaaaaaaaaa\n",
            "jonathanrknight\n",
            "picture\n",
            "makemescream\n",
            "depression  \n",
            "ferngully\n",
            "depression  \n",
            " thanks\n",
            "llisalang\n",
            "beeeeeeed\n",
            "fairyfreia          \n",
            "realworldmom\n",
            "radiogrinch \n",
            " with \n",
            "shopping\n",
            "houseeee\n",
            " sorite\n",
            "incoming\n",
            " thanks\n",
            "depression  \n",
            "scarych         \n",
            "name\n",
            " thanks\n",
            "depression  \n",
            "depression  \n",
            "reading\n",
            "may    \n",
            "quickshooter                \n",
            "around\n",
            " yup\n",
            "remysoon\n",
            "depression\n",
            "katelyniscool   \n",
            "depression  \n",
            "shaundiviney\n",
            " thanks\n",
            "comeagainjen   \n",
            "lonlonraanch\n",
            "spurofmoment\n",
            "  thirsty\n",
            "oneofusisfdup\n",
            "party \n",
            "  updates\n",
            "     subway\n",
            " whaaatever\n",
            "name\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5lXljaX4nx8"
      },
      "source": [
        "traning_df.to_csv('/content/Train-Dataset-prcessed.csv',index=False)\n",
        "testing_df.to_csv('/content/Test-Dataset-prcessed.csv',index=False)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS1_ntiM5itz"
      },
      "source": [
        "# Training & `RNN` model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCpz-8Tl6EX0",
        "outputId": "64807a6f-58ba-4026-8c6c-740da369c612"
      },
      "source": [
        "# import some importent library or packages \n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import time,sys,re,string\n",
        "import copy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pathlib\n",
        "import zipfile\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "\n",
        "import torch.optim as optim\n",
        "import pathlib\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "!pip install torchsummary\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "from sklearn.utils import shuffle\n",
        "!pip install torchviz\n",
        "from torchviz import make_dot, make_dot_from_trace\n",
        "try:\n",
        "  import contractions\n",
        "except:\n",
        "  !pip install contractions\n",
        "import contractions\n",
        "from torchtext.legacy.data import Field, TabularDataset, BucketIterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, Vectors, GloVe\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import nltk\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('averaged_perceptron_tagger')\n",
        "    nltk.data.find('brown')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    nltk.download('brown')\n",
        "    nltk.download('punkt')\n",
        "from nltk import sent_tokenize,word_tokenize\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.manual_seed(0)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.5.3)\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.7/dist-packages (0.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.9.0+cu102)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (3.7.4.3)\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fbf2c0dd5f0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4O9tu5u6LC6"
      },
      "source": [
        "traning_df = pd.read_csv('/content/Train-Dataset-prcessed.csv')\n",
        "testing_df = pd.read_csv('/content/Test-Dataset-prcessed.csv')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "kQmdyRqE6fwK",
        "outputId": "d344e220-1bc4-4196-dc79-1050f64711dc"
      },
      "source": [
        "traning_df"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>number_of_word</th>\n",
              "      <th>number_of_letter</th>\n",
              "      <th>tweets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>119</td>\n",
              "      <td>reneecarrollaz kylekashuv these snowflakes wou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>loving summer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>64</td>\n",
              "      <td>about to pass out super tiredgood night twitte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>32</td>\n",
              "      <td>photo have notgotaprayer me too</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>22</td>\n",
              "      <td>118</td>\n",
              "      <td>macitout and recording music here with nona he...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8239</th>\n",
              "      <td>1</td>\n",
              "      <td>45</td>\n",
              "      <td>266</td>\n",
              "      <td>genechat largepopulation studies that look for...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8240</th>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>166</td>\n",
              "      <td>i spent all of  grade crying for absolutely no...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8241</th>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>182</td>\n",
              "      <td>i mean i think i am just being funny but appar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8242</th>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>103</td>\n",
              "      <td>comeagainjen jenn i just wanted to say that i ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8243</th>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>46</td>\n",
              "      <td>blimey  predictive text is a bugger is not it</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8244 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      sentiment  ...                                             tweets\n",
              "0             1  ...  reneecarrollaz kylekashuv these snowflakes wou...\n",
              "1             0  ...                                      loving summer\n",
              "2             0  ...  about to pass out super tiredgood night twitte...\n",
              "3             0  ...                   photo have notgotaprayer me too \n",
              "4             0  ...  macitout and recording music here with nona he...\n",
              "...         ...  ...                                                ...\n",
              "8239          1  ...  genechat largepopulation studies that look for...\n",
              "8240          1  ...  i spent all of  grade crying for absolutely no...\n",
              "8241          1  ...  i mean i think i am just being funny but appar...\n",
              "8242          0  ...  comeagainjen jenn i just wanted to say that i ...\n",
              "8243          0  ...      blimey  predictive text is a bugger is not it\n",
              "\n",
              "[8244 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n0l74pAlyr_"
      },
      "source": [
        "label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)\n",
        "text_field = Field(tokenize='spacy', lower=True, include_lengths=True, batch_first=True)\n",
        "fields = [('sentiment', label_field),  ('tweets', text_field)]\n",
        "\n",
        "\n",
        "train_data = TabularDataset(path=\"/content/Train-Dataset-prcessed.csv\",\n",
        "                                 format=\"csv\",\n",
        "                                  fields=fields,\n",
        "                                 skip_header=True)\n",
        "\n",
        "valid_data = TabularDataset(path=\"/content/Test-Dataset-prcessed.csv\",\n",
        "                                 format=\"csv\",\n",
        "                                  fields=fields,\n",
        "                                 skip_header=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "train_iter = BucketIterator(train_data, batch_size=32, sort_key=lambda x: len(x.tweets),\n",
        "                            device=device, sort=True, sort_within_batch=True)\n",
        "valid_iter = BucketIterator(valid_data, batch_size=32, sort_key=lambda x: len(x.tweets),\n",
        "                            device=device, sort=True, sort_within_batch=True)\n",
        "\n",
        "\n",
        "text_field.build_vocab(train_data,) # min_freq=3,vectors = \"glove.6B.100d\"\n",
        "label_field.build_vocab(train_data)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GXT5L5ADSU3",
        "outputId": "e7030892-d38a-433f-ba0b-5822a0fbabf4"
      },
      "source": [
        "#No. of unique tokens in text\n",
        "print(\"Size of TEXT vocabulary:\",len(text_field.vocab))\n",
        "\n",
        "#No. of unique tokens in label\n",
        "print(\"Size of LABEL vocabulary:\",len(label_field.vocab))\n",
        "\n",
        "#Commonly used words\n",
        "print(text_field.vocab.freqs.most_common(10))  \n",
        "\n",
        "#Word dictionary\n",
        "print(text_field.vocab.stoi)   "
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of TEXT vocabulary: 64\n",
            "Size of LABEL vocabulary: 3\n",
            "[('8', 459), ('6', 446), ('7', 427), ('5', 407), ('9', 407), ('13', 379), ('10', 376), ('11', 369), ('12', 359), ('4', 329)]\n",
            "defaultdict(<bound method Vocab._default_unk_index of <torchtext.legacy.vocab.Vocab object at 0x7fbf0c7157d0>>, {'<unk>': 0, '<pad>': 1, '8': 2, '6': 3, '7': 4, '5': 5, '9': 6, '13': 7, '10': 8, '11': 9, '12': 10, '4': 11, '15': 12, '14': 13, '16': 14, '18': 15, '17': 16, '3': 17, '19': 18, '21': 19, '20': 20, '23': 21, '22': 22, '24': 23, '2': 24, '25': 25, '26': 26, '27': 27, '29': 28, '28': 29, '1': 30, '30': 31, '32': 32, '31': 33, '36': 34, '38': 35, '39': 36, '45': 37, '41': 38, '33': 39, '34': 40, '42': 41, '43': 42, '48': 43, '49': 44, '44': 45, '47': 46, '46': 47, '37': 48, '40': 49, '35': 50, '50': 51, '52': 52, '51': 53, '53': 54, '54': 55, '55': 56, '56': 57, '58': 58, '61': 59, '57': 60, '59': 61, '60': 62, '62': 63})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5efYiqDDDfD"
      },
      "source": [
        "## RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf-hTYsT4TH-"
      },
      "source": [
        "import torch.nn as nn\n",
        "class RNNModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, 2, batch_first=True, nonlinearity='relu')\n",
        "        self.fc = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        # print(len(sentence))\n",
        "        # x = embeds.view(len(sentence), 1, -1)\n",
        "         # Initialize hidden state with zeros\n",
        "        h0 = torch.autograd.Variable(torch.zeros(2, sentence.size(0), self.hidden_dim))\n",
        "            \n",
        "        out, hn = self.rnn(embeds, h0)\n",
        "        out = self.fc(out[:, -1, :]) \n",
        "        return out\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OQmhLFsFY21"
      },
      "source": [
        "EMBEDDING_DIM = 6\n",
        "HIDDEN_DIM = 6\n",
        "model = RNNModel(EMBEDDING_DIM, HIDDEN_DIM, len(text_field.vocab), 1)\n",
        "loss_function = torch.nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhleHCW344v2"
      },
      "source": [
        "def binary_accuracy(preds, y):\n",
        "\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWHp10bmBKv0"
      },
      "source": [
        "# training function \n",
        "def train(model, iterator):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for i,batch in enumerate( iterator,1):\n",
        "        tweets, tweets_lengths = batch.tweets\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(tweets,).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.sentiment)\n",
        "        acc = binary_accuracy(predictions, batch.sentiment)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "        print_val = f\"running_loss : {(loss.item()):.6f}\\t\"\n",
        "        print_val += f\"running_corrects : {acc.item():.6f}\\t\"  \n",
        "        sys.stdout.write('\\r' + str(print_val))\n",
        "        \n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mS4_HLBXFwF6"
      },
      "source": [
        "def evaluate(model, iterator):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            tweets, tweets_lengths = batch.tweets\n",
        "            predictions = model(tweets,).squeeze(1)\n",
        "            loss = criterion(predictions, batch.sentiment)\n",
        "            acc = binary_accuracy(predictions, batch.sentiment)\n",
        "            \n",
        "            epoch_acc += acc.item()\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return  epoch_loss / len(iterator), epoch_acc / len(iterator) "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcKeZOYxF4OA",
        "outputId": "66cc4600-d8ca-4314-a6e8-46d3298df3d0"
      },
      "source": [
        "t = time.time()\n",
        "loss=[]\n",
        "acc=[]\n",
        "val_acc=[]\n",
        "val_loss=[]\n",
        "num_epochs = 500\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_iter)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iter)\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%  | Val. Acc: {valid_acc*100:.2f}%')\n",
        "    # print(f'\\t')\n",
        "    \n",
        "    loss.append(train_loss)\n",
        "    acc.append(train_acc)\n",
        "    val_loss.append(valid_loss)\n",
        "    val_acc.append(valid_acc)\n",
        "    \n",
        "print(f'time:{time.time()-t:.3f}')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running_loss : 0.620854\trunning_corrects : 0.650000\t\tTrain Loss: 0.625 | Train Acc: 68.03%  | Val. Acc: 77.74%\n",
            "running_loss : 0.642528\trunning_corrects : 0.650000\t\tTrain Loss: 0.534 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.648882\trunning_corrects : 0.650000\t\tTrain Loss: 0.526 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.648717\trunning_corrects : 0.650000\t\tTrain Loss: 0.525 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.647076\trunning_corrects : 0.650000\t\tTrain Loss: 0.523 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.645144\trunning_corrects : 0.650000\t\tTrain Loss: 0.523 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.643093\trunning_corrects : 0.650000\t\tTrain Loss: 0.522 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.640952\trunning_corrects : 0.650000\t\tTrain Loss: 0.521 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.638713\trunning_corrects : 0.650000\t\tTrain Loss: 0.520 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.636268\trunning_corrects : 0.650000\t\tTrain Loss: 0.519 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.633782\trunning_corrects : 0.650000\t\tTrain Loss: 0.518 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.631173\trunning_corrects : 0.650000\t\tTrain Loss: 0.517 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.628342\trunning_corrects : 0.650000\t\tTrain Loss: 0.517 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.625692\trunning_corrects : 0.650000\t\tTrain Loss: 0.516 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.622954\trunning_corrects : 0.650000\t\tTrain Loss: 0.515 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.620292\trunning_corrects : 0.650000\t\tTrain Loss: 0.515 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.617673\trunning_corrects : 0.650000\t\tTrain Loss: 0.514 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.615419\trunning_corrects : 0.650000\t\tTrain Loss: 0.513 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.613422\trunning_corrects : 0.650000\t\tTrain Loss: 0.513 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.611473\trunning_corrects : 0.650000\t\tTrain Loss: 0.512 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.609521\trunning_corrects : 0.650000\t\tTrain Loss: 0.511 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.607839\trunning_corrects : 0.650000\t\tTrain Loss: 0.511 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.605938\trunning_corrects : 0.650000\t\tTrain Loss: 0.510 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.604087\trunning_corrects : 0.650000\t\tTrain Loss: 0.510 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.602299\trunning_corrects : 0.650000\t\tTrain Loss: 0.509 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.600475\trunning_corrects : 0.650000\t\tTrain Loss: 0.508 | Train Acc: 77.58%  | Val. Acc: 77.74%\n",
            "running_loss : 0.598761\trunning_corrects : 0.650000\t\tTrain Loss: 0.508 | Train Acc: 77.59%  | Val. Acc: 77.74%\n",
            "running_loss : 0.597410\trunning_corrects : 0.650000\t\tTrain Loss: 0.507 | Train Acc: 77.67%  | Val. Acc: 77.94%\n",
            "running_loss : 0.596304\trunning_corrects : 0.650000\t\tTrain Loss: 0.506 | Train Acc: 77.82%  | Val. Acc: 78.18%\n",
            "running_loss : 0.595262\trunning_corrects : 0.650000\t\tTrain Loss: 0.506 | Train Acc: 77.93%  | Val. Acc: 78.23%\n",
            "running_loss : 0.593809\trunning_corrects : 0.650000\t\tTrain Loss: 0.505 | Train Acc: 77.99%  | Val. Acc: 78.23%\n",
            "running_loss : 0.592480\trunning_corrects : 0.650000\t\tTrain Loss: 0.504 | Train Acc: 78.03%  | Val. Acc: 78.27%\n",
            "running_loss : 0.591358\trunning_corrects : 0.650000\t\tTrain Loss: 0.503 | Train Acc: 78.09%  | Val. Acc: 78.47%\n",
            "running_loss : 0.590255\trunning_corrects : 0.650000\t\tTrain Loss: 0.502 | Train Acc: 78.23%  | Val. Acc: 78.47%\n",
            "running_loss : 0.588629\trunning_corrects : 0.650000\t\tTrain Loss: 0.502 | Train Acc: 78.36%  | Val. Acc: 78.66%\n",
            "running_loss : 0.586505\trunning_corrects : 0.650000\t\tTrain Loss: 0.501 | Train Acc: 78.46%  | Val. Acc: 78.66%\n",
            "running_loss : 0.583271\trunning_corrects : 0.650000\t\tTrain Loss: 0.500 | Train Acc: 78.50%  | Val. Acc: 79.09%\n",
            "running_loss : 0.579902\trunning_corrects : 0.650000\t\tTrain Loss: 0.499 | Train Acc: 78.64%  | Val. Acc: 78.23%\n",
            "running_loss : 0.577295\trunning_corrects : 0.650000\t\tTrain Loss: 0.497 | Train Acc: 78.40%  | Val. Acc: 78.85%\n",
            "running_loss : 0.574449\trunning_corrects : 0.650000\t\tTrain Loss: 0.496 | Train Acc: 78.78%  | Val. Acc: 79.67%\n",
            "running_loss : 0.571448\trunning_corrects : 0.650000\t\tTrain Loss: 0.495 | Train Acc: 79.29%  | Val. Acc: 80.00%\n",
            "running_loss : 0.568242\trunning_corrects : 0.650000\t\tTrain Loss: 0.494 | Train Acc: 79.65%  | Val. Acc: 80.10%\n",
            "running_loss : 0.564681\trunning_corrects : 0.650000\t\tTrain Loss: 0.492 | Train Acc: 79.76%  | Val. Acc: 80.10%\n",
            "running_loss : 0.560946\trunning_corrects : 0.650000\t\tTrain Loss: 0.491 | Train Acc: 79.76%  | Val. Acc: 80.10%\n",
            "running_loss : 0.557597\trunning_corrects : 0.650000\t\tTrain Loss: 0.490 | Train Acc: 79.76%  | Val. Acc: 80.10%\n",
            "running_loss : 0.554658\trunning_corrects : 0.650000\t\tTrain Loss: 0.489 | Train Acc: 79.76%  | Val. Acc: 80.10%\n",
            "running_loss : 0.551957\trunning_corrects : 0.650000\t\tTrain Loss: 0.488 | Train Acc: 79.76%  | Val. Acc: 80.10%\n",
            "running_loss : 0.549298\trunning_corrects : 0.650000\t\tTrain Loss: 0.487 | Train Acc: 79.77%  | Val. Acc: 79.86%\n",
            "running_loss : 0.546789\trunning_corrects : 0.700000\t\tTrain Loss: 0.485 | Train Acc: 79.80%  | Val. Acc: 79.86%\n",
            "running_loss : 0.543871\trunning_corrects : 0.700000\t\tTrain Loss: 0.484 | Train Acc: 79.81%  | Val. Acc: 79.86%\n",
            "running_loss : 0.540465\trunning_corrects : 0.750000\t\tTrain Loss: 0.483 | Train Acc: 79.94%  | Val. Acc: 80.05%\n",
            "running_loss : 0.537380\trunning_corrects : 0.750000\t\tTrain Loss: 0.482 | Train Acc: 80.09%  | Val. Acc: 80.39%\n",
            "running_loss : 0.534327\trunning_corrects : 0.750000\t\tTrain Loss: 0.481 | Train Acc: 80.28%  | Val. Acc: 80.39%\n",
            "running_loss : 0.531326\trunning_corrects : 0.750000\t\tTrain Loss: 0.480 | Train Acc: 80.35%  | Val. Acc: 80.39%\n",
            "running_loss : 0.528167\trunning_corrects : 0.750000\t\tTrain Loss: 0.479 | Train Acc: 80.35%  | Val. Acc: 80.39%\n",
            "running_loss : 0.524394\trunning_corrects : 0.750000\t\tTrain Loss: 0.478 | Train Acc: 80.38%  | Val. Acc: 80.49%\n",
            "running_loss : 0.521269\trunning_corrects : 0.750000\t\tTrain Loss: 0.477 | Train Acc: 80.52%  | Val. Acc: 80.49%\n",
            "running_loss : 0.518444\trunning_corrects : 0.750000\t\tTrain Loss: 0.476 | Train Acc: 80.60%  | Val. Acc: 80.49%\n",
            "running_loss : 0.515274\trunning_corrects : 0.750000\t\tTrain Loss: 0.475 | Train Acc: 80.68%  | Val. Acc: 80.49%\n",
            "running_loss : 0.510359\trunning_corrects : 0.750000\t\tTrain Loss: 0.474 | Train Acc: 80.68%  | Val. Acc: 80.49%\n",
            "running_loss : 0.503985\trunning_corrects : 0.750000\t\tTrain Loss: 0.473 | Train Acc: 80.68%  | Val. Acc: 80.49%\n",
            "running_loss : 0.498755\trunning_corrects : 0.800000\t\tTrain Loss: 0.472 | Train Acc: 80.78%  | Val. Acc: 80.73%\n",
            "running_loss : 0.493920\trunning_corrects : 0.800000\t\tTrain Loss: 0.471 | Train Acc: 81.00%  | Val. Acc: 80.92%\n",
            "running_loss : 0.489888\trunning_corrects : 0.800000\t\tTrain Loss: 0.470 | Train Acc: 81.22%  | Val. Acc: 80.92%\n",
            "running_loss : 0.488205\trunning_corrects : 0.800000\t\tTrain Loss: 0.469 | Train Acc: 81.32%  | Val. Acc: 80.92%\n",
            "running_loss : 0.485841\trunning_corrects : 0.800000\t\tTrain Loss: 0.468 | Train Acc: 81.37%  | Val. Acc: 80.92%\n",
            "running_loss : 0.484085\trunning_corrects : 0.800000\t\tTrain Loss: 0.467 | Train Acc: 81.72%  | Val. Acc: 81.06%\n",
            "running_loss : 0.481770\trunning_corrects : 0.800000\t\tTrain Loss: 0.466 | Train Acc: 82.12%  | Val. Acc: 81.93%\n",
            "running_loss : 0.480818\trunning_corrects : 0.800000\t\tTrain Loss: 0.465 | Train Acc: 82.20%  | Val. Acc: 81.93%\n",
            "running_loss : 0.478361\trunning_corrects : 0.800000\t\tTrain Loss: 0.464 | Train Acc: 82.21%  | Val. Acc: 81.93%\n",
            "running_loss : 0.475653\trunning_corrects : 0.800000\t\tTrain Loss: 0.463 | Train Acc: 82.23%  | Val. Acc: 81.93%\n",
            "running_loss : 0.473185\trunning_corrects : 0.800000\t\tTrain Loss: 0.462 | Train Acc: 82.29%  | Val. Acc: 81.93%\n",
            "running_loss : 0.470747\trunning_corrects : 0.800000\t\tTrain Loss: 0.462 | Train Acc: 82.37%  | Val. Acc: 82.22%\n",
            "running_loss : 0.468506\trunning_corrects : 0.800000\t\tTrain Loss: 0.461 | Train Acc: 82.48%  | Val. Acc: 82.22%\n",
            "running_loss : 0.466234\trunning_corrects : 0.800000\t\tTrain Loss: 0.460 | Train Acc: 82.48%  | Val. Acc: 82.22%\n",
            "running_loss : 0.463902\trunning_corrects : 0.800000\t\tTrain Loss: 0.459 | Train Acc: 82.48%  | Val. Acc: 82.22%\n",
            "running_loss : 0.461587\trunning_corrects : 0.800000\t\tTrain Loss: 0.459 | Train Acc: 82.48%  | Val. Acc: 82.22%\n",
            "running_loss : 0.459215\trunning_corrects : 0.800000\t\tTrain Loss: 0.458 | Train Acc: 82.48%  | Val. Acc: 82.22%\n",
            "running_loss : 0.456898\trunning_corrects : 0.800000\t\tTrain Loss: 0.458 | Train Acc: 82.48%  | Val. Acc: 82.22%\n",
            "running_loss : 0.455115\trunning_corrects : 0.800000\t\tTrain Loss: 0.457 | Train Acc: 82.50%  | Val. Acc: 82.31%\n",
            "running_loss : 0.453526\trunning_corrects : 0.850000\t\tTrain Loss: 0.457 | Train Acc: 82.63%  | Val. Acc: 82.31%\n",
            "running_loss : 0.451982\trunning_corrects : 0.850000\t\tTrain Loss: 0.456 | Train Acc: 82.68%  | Val. Acc: 82.31%\n",
            "running_loss : 0.450163\trunning_corrects : 0.850000\t\tTrain Loss: 0.456 | Train Acc: 82.81%  | Val. Acc: 82.31%\n",
            "running_loss : 0.448666\trunning_corrects : 0.850000\t\tTrain Loss: 0.455 | Train Acc: 82.81%  | Val. Acc: 82.31%\n",
            "running_loss : 0.447166\trunning_corrects : 0.850000\t\tTrain Loss: 0.455 | Train Acc: 82.81%  | Val. Acc: 82.31%\n",
            "running_loss : 0.445671\trunning_corrects : 0.850000\t\tTrain Loss: 0.455 | Train Acc: 82.81%  | Val. Acc: 82.31%\n",
            "running_loss : 0.444386\trunning_corrects : 0.850000\t\tTrain Loss: 0.454 | Train Acc: 82.85%  | Val. Acc: 82.31%\n",
            "running_loss : 0.443076\trunning_corrects : 0.850000\t\tTrain Loss: 0.454 | Train Acc: 82.89%  | Val. Acc: 82.31%\n",
            "running_loss : 0.441919\trunning_corrects : 0.850000\t\tTrain Loss: 0.454 | Train Acc: 82.89%  | Val. Acc: 82.55%\n",
            "running_loss : 0.441058\trunning_corrects : 0.850000\t\tTrain Loss: 0.453 | Train Acc: 82.96%  | Val. Acc: 82.55%\n",
            "running_loss : 0.440145\trunning_corrects : 0.850000\t\tTrain Loss: 0.453 | Train Acc: 83.00%  | Val. Acc: 82.55%\n",
            "running_loss : 0.439133\trunning_corrects : 0.850000\t\tTrain Loss: 0.453 | Train Acc: 83.01%  | Val. Acc: 82.55%\n",
            "running_loss : 0.438073\trunning_corrects : 0.850000\t\tTrain Loss: 0.452 | Train Acc: 83.05%  | Val. Acc: 82.55%\n",
            "running_loss : 0.437065\trunning_corrects : 0.850000\t\tTrain Loss: 0.452 | Train Acc: 83.05%  | Val. Acc: 82.55%\n",
            "running_loss : 0.435951\trunning_corrects : 0.850000\t\tTrain Loss: 0.452 | Train Acc: 83.07%  | Val. Acc: 82.55%\n",
            "running_loss : 0.435303\trunning_corrects : 0.850000\t\tTrain Loss: 0.451 | Train Acc: 83.07%  | Val. Acc: 82.55%\n",
            "running_loss : 0.434568\trunning_corrects : 0.850000\t\tTrain Loss: 0.451 | Train Acc: 83.07%  | Val. Acc: 82.55%\n",
            "running_loss : 0.433824\trunning_corrects : 0.850000\t\tTrain Loss: 0.451 | Train Acc: 83.07%  | Val. Acc: 82.55%\n",
            "running_loss : 0.433276\trunning_corrects : 0.850000\t\tTrain Loss: 0.451 | Train Acc: 83.07%  | Val. Acc: 82.55%\n",
            "running_loss : 0.432837\trunning_corrects : 0.850000\t\tTrain Loss: 0.450 | Train Acc: 83.07%  | Val. Acc: 82.55%\n",
            "running_loss : 0.432480\trunning_corrects : 0.850000\t\tTrain Loss: 0.450 | Train Acc: 83.07%  | Val. Acc: 82.55%\n",
            "running_loss : 0.431939\trunning_corrects : 0.850000\t\tTrain Loss: 0.450 | Train Acc: 83.07%  | Val. Acc: 82.55%\n",
            "running_loss : 0.431545\trunning_corrects : 0.850000\t\tTrain Loss: 0.450 | Train Acc: 83.07%  | Val. Acc: 82.55%\n",
            "running_loss : 0.431111\trunning_corrects : 0.850000\t\tTrain Loss: 0.449 | Train Acc: 83.07%  | Val. Acc: 82.55%\n",
            "running_loss : 0.430592\trunning_corrects : 0.850000\t\tTrain Loss: 0.449 | Train Acc: 83.07%  | Val. Acc: 82.55%\n",
            "running_loss : 0.429243\trunning_corrects : 0.850000\t\tTrain Loss: 0.449 | Train Acc: 83.07%  | Val. Acc: 82.55%\n",
            "running_loss : 0.428988\trunning_corrects : 0.850000\t\tTrain Loss: 0.449 | Train Acc: 83.07%  | Val. Acc: 82.55%\n",
            "running_loss : 0.428695\trunning_corrects : 0.850000\t\tTrain Loss: 0.448 | Train Acc: 83.07%  | Val. Acc: 82.65%\n",
            "running_loss : 0.428273\trunning_corrects : 0.850000\t\tTrain Loss: 0.448 | Train Acc: 83.09%  | Val. Acc: 82.65%\n",
            "running_loss : 0.428001\trunning_corrects : 0.850000\t\tTrain Loss: 0.448 | Train Acc: 83.11%  | Val. Acc: 82.65%\n",
            "running_loss : 0.427653\trunning_corrects : 0.850000\t\tTrain Loss: 0.448 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.427257\trunning_corrects : 0.850000\t\tTrain Loss: 0.448 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.427108\trunning_corrects : 0.850000\t\tTrain Loss: 0.447 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.426826\trunning_corrects : 0.850000\t\tTrain Loss: 0.447 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.426485\trunning_corrects : 0.850000\t\tTrain Loss: 0.447 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.426477\trunning_corrects : 0.850000\t\tTrain Loss: 0.447 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.426430\trunning_corrects : 0.850000\t\tTrain Loss: 0.447 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.426186\trunning_corrects : 0.850000\t\tTrain Loss: 0.447 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.426189\trunning_corrects : 0.850000\t\tTrain Loss: 0.446 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.426036\trunning_corrects : 0.850000\t\tTrain Loss: 0.446 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.425691\trunning_corrects : 0.850000\t\tTrain Loss: 0.446 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.425481\trunning_corrects : 0.850000\t\tTrain Loss: 0.446 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.425259\trunning_corrects : 0.850000\t\tTrain Loss: 0.446 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.424138\trunning_corrects : 0.850000\t\tTrain Loss: 0.446 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.423836\trunning_corrects : 0.850000\t\tTrain Loss: 0.445 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.423689\trunning_corrects : 0.850000\t\tTrain Loss: 0.445 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.423539\trunning_corrects : 0.850000\t\tTrain Loss: 0.445 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.422073\trunning_corrects : 0.850000\t\tTrain Loss: 0.445 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.421244\trunning_corrects : 0.850000\t\tTrain Loss: 0.444 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.421199\trunning_corrects : 0.850000\t\tTrain Loss: 0.444 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.421244\trunning_corrects : 0.850000\t\tTrain Loss: 0.444 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.421302\trunning_corrects : 0.850000\t\tTrain Loss: 0.444 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.421000\trunning_corrects : 0.850000\t\tTrain Loss: 0.444 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.421714\trunning_corrects : 0.850000\t\tTrain Loss: 0.443 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.421267\trunning_corrects : 0.850000\t\tTrain Loss: 0.443 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.421156\trunning_corrects : 0.850000\t\tTrain Loss: 0.443 | Train Acc: 83.12%  | Val. Acc: 82.65%\n",
            "running_loss : 0.421491\trunning_corrects : 0.850000\t\tTrain Loss: 0.443 | Train Acc: 83.13%  | Val. Acc: 82.65%\n",
            "running_loss : 0.421632\trunning_corrects : 0.850000\t\tTrain Loss: 0.443 | Train Acc: 83.21%  | Val. Acc: 82.84%\n",
            "running_loss : 0.422169\trunning_corrects : 0.850000\t\tTrain Loss: 0.442 | Train Acc: 83.31%  | Val. Acc: 82.84%\n",
            "running_loss : 0.422230\trunning_corrects : 0.850000\t\tTrain Loss: 0.442 | Train Acc: 83.31%  | Val. Acc: 82.84%\n",
            "running_loss : 0.422554\trunning_corrects : 0.850000\t\tTrain Loss: 0.442 | Train Acc: 83.31%  | Val. Acc: 82.84%\n",
            "running_loss : 0.422680\trunning_corrects : 0.850000\t\tTrain Loss: 0.442 | Train Acc: 83.31%  | Val. Acc: 82.84%\n",
            "running_loss : 0.422513\trunning_corrects : 0.850000\t\tTrain Loss: 0.442 | Train Acc: 83.31%  | Val. Acc: 82.84%\n",
            "running_loss : 0.422609\trunning_corrects : 0.850000\t\tTrain Loss: 0.442 | Train Acc: 83.31%  | Val. Acc: 82.84%\n",
            "running_loss : 0.422712\trunning_corrects : 0.850000\t\tTrain Loss: 0.441 | Train Acc: 83.31%  | Val. Acc: 82.84%\n",
            "running_loss : 0.422915\trunning_corrects : 0.850000\t\tTrain Loss: 0.441 | Train Acc: 83.31%  | Val. Acc: 82.84%\n",
            "running_loss : 0.422730\trunning_corrects : 0.850000\t\tTrain Loss: 0.441 | Train Acc: 83.31%  | Val. Acc: 82.84%\n",
            "running_loss : 0.422678\trunning_corrects : 0.850000\t\tTrain Loss: 0.441 | Train Acc: 83.31%  | Val. Acc: 82.84%\n",
            "running_loss : 0.422739\trunning_corrects : 0.850000\t\tTrain Loss: 0.441 | Train Acc: 83.31%  | Val. Acc: 82.84%\n",
            "running_loss : 0.422753\trunning_corrects : 0.850000\t\tTrain Loss: 0.441 | Train Acc: 83.31%  | Val. Acc: 82.84%\n",
            "running_loss : 0.422957\trunning_corrects : 0.850000\t\tTrain Loss: 0.441 | Train Acc: 83.31%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423012\trunning_corrects : 0.850000\t\tTrain Loss: 0.441 | Train Acc: 83.31%  | Val. Acc: 82.89%\n",
            "running_loss : 0.422746\trunning_corrects : 0.850000\t\tTrain Loss: 0.441 | Train Acc: 83.32%  | Val. Acc: 82.89%\n",
            "running_loss : 0.422659\trunning_corrects : 0.850000\t\tTrain Loss: 0.440 | Train Acc: 83.32%  | Val. Acc: 82.89%\n",
            "running_loss : 0.422963\trunning_corrects : 0.850000\t\tTrain Loss: 0.440 | Train Acc: 83.32%  | Val. Acc: 82.89%\n",
            "running_loss : 0.422888\trunning_corrects : 0.850000\t\tTrain Loss: 0.440 | Train Acc: 83.32%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423057\trunning_corrects : 0.850000\t\tTrain Loss: 0.440 | Train Acc: 83.32%  | Val. Acc: 82.89%\n",
            "running_loss : 0.422881\trunning_corrects : 0.850000\t\tTrain Loss: 0.440 | Train Acc: 83.32%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423172\trunning_corrects : 0.850000\t\tTrain Loss: 0.440 | Train Acc: 83.32%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423157\trunning_corrects : 0.850000\t\tTrain Loss: 0.440 | Train Acc: 83.32%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423214\trunning_corrects : 0.850000\t\tTrain Loss: 0.440 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423043\trunning_corrects : 0.850000\t\tTrain Loss: 0.440 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423001\trunning_corrects : 0.850000\t\tTrain Loss: 0.440 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423394\trunning_corrects : 0.850000\t\tTrain Loss: 0.440 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423213\trunning_corrects : 0.850000\t\tTrain Loss: 0.440 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423213\trunning_corrects : 0.850000\t\tTrain Loss: 0.440 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423374\trunning_corrects : 0.850000\t\tTrain Loss: 0.439 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423399\trunning_corrects : 0.850000\t\tTrain Loss: 0.439 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423337\trunning_corrects : 0.850000\t\tTrain Loss: 0.439 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423239\trunning_corrects : 0.850000\t\tTrain Loss: 0.439 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423413\trunning_corrects : 0.850000\t\tTrain Loss: 0.439 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423399\trunning_corrects : 0.850000\t\tTrain Loss: 0.439 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423790\trunning_corrects : 0.850000\t\tTrain Loss: 0.439 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423491\trunning_corrects : 0.850000\t\tTrain Loss: 0.439 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423562\trunning_corrects : 0.850000\t\tTrain Loss: 0.439 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423550\trunning_corrects : 0.850000\t\tTrain Loss: 0.439 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423568\trunning_corrects : 0.850000\t\tTrain Loss: 0.439 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423846\trunning_corrects : 0.850000\t\tTrain Loss: 0.439 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423582\trunning_corrects : 0.850000\t\tTrain Loss: 0.439 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423781\trunning_corrects : 0.850000\t\tTrain Loss: 0.439 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423421\trunning_corrects : 0.850000\t\tTrain Loss: 0.439 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423801\trunning_corrects : 0.850000\t\tTrain Loss: 0.439 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423829\trunning_corrects : 0.850000\t\tTrain Loss: 0.439 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423863\trunning_corrects : 0.850000\t\tTrain Loss: 0.439 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423809\trunning_corrects : 0.850000\t\tTrain Loss: 0.439 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.423942\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424009\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424131\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424097\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424134\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424205\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424229\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424220\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424314\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424345\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424142\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424488\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424670\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424678\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424589\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424737\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424657\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424597\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424872\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424774\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424807\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424902\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424738\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.425007\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.425124\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424966\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.425000\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.425101\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.424998\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.34%  | Val. Acc: 82.89%\n",
            "running_loss : 0.425058\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.35%  | Val. Acc: 82.99%\n",
            "running_loss : 0.425208\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.35%  | Val. Acc: 82.99%\n",
            "running_loss : 0.425202\trunning_corrects : 0.850000\t\tTrain Loss: 0.438 | Train Acc: 83.35%  | Val. Acc: 82.99%\n",
            "running_loss : 0.425160\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.425421\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.425483\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.425483\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.425676\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.425616\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.425940\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426009\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426080\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.425924\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426182\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426056\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426187\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426456\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426440\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426503\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426324\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426410\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426384\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426399\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426597\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426650\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426649\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426751\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426690\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.36%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426780\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426625\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426732\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426809\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426570\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426953\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.427080\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.427011\trunning_corrects : 0.850000\t\tTrain Loss: 0.437 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.427202\trunning_corrects : 0.850000\t\tTrain Loss: 0.436 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.427107\trunning_corrects : 0.850000\t\tTrain Loss: 0.436 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.427069\trunning_corrects : 0.850000\t\tTrain Loss: 0.436 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.427025\trunning_corrects : 0.850000\t\tTrain Loss: 0.436 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.427075\trunning_corrects : 0.850000\t\tTrain Loss: 0.436 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.427106\trunning_corrects : 0.850000\t\tTrain Loss: 0.436 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.427135\trunning_corrects : 0.850000\t\tTrain Loss: 0.436 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.427071\trunning_corrects : 0.850000\t\tTrain Loss: 0.436 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.427218\trunning_corrects : 0.850000\t\tTrain Loss: 0.436 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.427000\trunning_corrects : 0.850000\t\tTrain Loss: 0.436 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.427143\trunning_corrects : 0.850000\t\tTrain Loss: 0.436 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.427239\trunning_corrects : 0.850000\t\tTrain Loss: 0.436 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.427274\trunning_corrects : 0.850000\t\tTrain Loss: 0.436 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.427343\trunning_corrects : 0.850000\t\tTrain Loss: 0.436 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.427212\trunning_corrects : 0.850000\t\tTrain Loss: 0.436 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.426737\trunning_corrects : 0.850000\t\tTrain Loss: 0.436 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.425580\trunning_corrects : 0.850000\t\tTrain Loss: 0.436 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.424877\trunning_corrects : 0.850000\t\tTrain Loss: 0.436 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.424680\trunning_corrects : 0.850000\t\tTrain Loss: 0.436 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.424642\trunning_corrects : 0.850000\t\tTrain Loss: 0.435 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.424831\trunning_corrects : 0.850000\t\tTrain Loss: 0.435 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.425042\trunning_corrects : 0.850000\t\tTrain Loss: 0.435 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.424933\trunning_corrects : 0.850000\t\tTrain Loss: 0.435 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.425028\trunning_corrects : 0.850000\t\tTrain Loss: 0.435 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.424877\trunning_corrects : 0.850000\t\tTrain Loss: 0.435 | Train Acc: 83.37%  | Val. Acc: 82.99%\n",
            "running_loss : 0.425119\trunning_corrects : 0.850000\t\tTrain Loss: 0.435 | Train Acc: 83.38%  | Val. Acc: 82.99%\n",
            "running_loss : 0.425205\trunning_corrects : 0.850000\t\tTrain Loss: 0.435 | Train Acc: 83.46%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424913\trunning_corrects : 0.850000\t\tTrain Loss: 0.435 | Train Acc: 83.54%  | Val. Acc: 83.13%\n",
            "running_loss : 0.425065\trunning_corrects : 0.850000\t\tTrain Loss: 0.434 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424861\trunning_corrects : 0.850000\t\tTrain Loss: 0.434 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424949\trunning_corrects : 0.850000\t\tTrain Loss: 0.434 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424653\trunning_corrects : 0.850000\t\tTrain Loss: 0.434 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424942\trunning_corrects : 0.850000\t\tTrain Loss: 0.434 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424924\trunning_corrects : 0.850000\t\tTrain Loss: 0.434 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424677\trunning_corrects : 0.850000\t\tTrain Loss: 0.434 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424918\trunning_corrects : 0.850000\t\tTrain Loss: 0.434 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424847\trunning_corrects : 0.850000\t\tTrain Loss: 0.434 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424606\trunning_corrects : 0.850000\t\tTrain Loss: 0.434 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424796\trunning_corrects : 0.850000\t\tTrain Loss: 0.434 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424667\trunning_corrects : 0.850000\t\tTrain Loss: 0.434 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424724\trunning_corrects : 0.850000\t\tTrain Loss: 0.434 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424729\trunning_corrects : 0.850000\t\tTrain Loss: 0.434 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424582\trunning_corrects : 0.850000\t\tTrain Loss: 0.434 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424888\trunning_corrects : 0.850000\t\tTrain Loss: 0.434 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424954\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424746\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424825\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424944\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.425092\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424832\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424752\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424770\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424688\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424535\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424615\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424365\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424462\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424310\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424572\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424209\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424472\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424914\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424585\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424540\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424433\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424419\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424280\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424147\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.423951\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424014\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.423725\trunning_corrects : 0.850000\t\tTrain Loss: 0.432 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424075\trunning_corrects : 0.850000\t\tTrain Loss: 0.433 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.423894\trunning_corrects : 0.850000\t\tTrain Loss: 0.432 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.423870\trunning_corrects : 0.850000\t\tTrain Loss: 0.432 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.423778\trunning_corrects : 0.850000\t\tTrain Loss: 0.432 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.423796\trunning_corrects : 0.850000\t\tTrain Loss: 0.432 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.423812\trunning_corrects : 0.850000\t\tTrain Loss: 0.432 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.423772\trunning_corrects : 0.850000\t\tTrain Loss: 0.432 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.423789\trunning_corrects : 0.850000\t\tTrain Loss: 0.432 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.423743\trunning_corrects : 0.850000\t\tTrain Loss: 0.432 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.422980\trunning_corrects : 0.850000\t\tTrain Loss: 0.432 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.423948\trunning_corrects : 0.850000\t\tTrain Loss: 0.432 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.423204\trunning_corrects : 0.850000\t\tTrain Loss: 0.432 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.423261\trunning_corrects : 0.850000\t\tTrain Loss: 0.432 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.423847\trunning_corrects : 0.850000\t\tTrain Loss: 0.432 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424422\trunning_corrects : 0.850000\t\tTrain Loss: 0.432 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424526\trunning_corrects : 0.850000\t\tTrain Loss: 0.432 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.424957\trunning_corrects : 0.850000\t\tTrain Loss: 0.432 | Train Acc: 83.55%  | Val. Acc: 83.13%\n",
            "running_loss : 0.425428\trunning_corrects : 0.850000\t\tTrain Loss: 0.432 | Train Acc: 83.57%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425656\trunning_corrects : 0.850000\t\tTrain Loss: 0.432 | Train Acc: 83.61%  | Val. Acc: 83.23%\n",
            "running_loss : 0.426119\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.426303\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.426670\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.426591\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.426960\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.426595\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.427018\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.426850\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.426777\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.426540\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.426373\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.426911\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.426715\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.426268\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.426306\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.426001\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425635\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425622\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425443\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425283\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425343\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425455\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425242\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425131\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425182\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425152\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.424953\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425211\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425113\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.424957\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425106\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425175\trunning_corrects : 0.850000\t\tTrain Loss: 0.431 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.424824\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425284\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.65%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425078\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.66%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425081\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.66%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425025\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.66%  | Val. Acc: 83.23%\n",
            "running_loss : 0.424910\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.66%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425093\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.66%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425228\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425002\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425213\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425154\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425207\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425210\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425007\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425138\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425153\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.424987\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425143\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425035\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425130\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425037\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425058\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425220\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425127\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425090\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425138\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425389\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425100\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425346\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425193\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425443\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425142\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425179\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425198\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425250\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425365\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425359\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425324\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425255\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425315\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425398\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425430\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425304\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425056\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425068\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425393\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425413\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425382\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425263\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425562\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425423\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425325\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425670\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425524\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425472\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425392\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425314\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425466\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425413\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425749\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425600\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425525\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425524\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425569\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425386\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425518\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425345\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425416\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425402\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425612\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425660\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425496\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425609\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425547\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425481\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425564\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425617\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425464\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425915\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425747\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425713\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425565\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425708\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425467\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425670\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425694\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425619\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425446\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425631\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425633\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425810\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425795\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425735\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425792\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425762\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425665\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425778\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425812\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425730\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425817\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425676\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425712\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425914\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425932\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425762\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425939\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425872\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425979\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425725\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425985\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425955\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425891\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425775\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425957\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425966\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425462\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425681\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.426176\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.426138\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425531\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.426078\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.426092\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425919\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425582\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425595\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425538\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425629\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "running_loss : 0.425981\trunning_corrects : 0.850000\t\tTrain Loss: 0.430 | Train Acc: 83.67%  | Val. Acc: 83.23%\n",
            "time:277.988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo5j5q2H2IZZ"
      },
      "source": [
        "# Classification Performance Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaNZD97cLleV"
      },
      "source": [
        "# import some importent library or packages \n",
        "import glob,sys,os\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import numpy as np\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import pathlib\n",
        "import zipfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import torch.optim as optim\n",
        "import time,sys\n",
        "import copy\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQJ35QIc2L3Z"
      },
      "source": [
        "_tranning_loss = loss\n",
        "_tranning_acc = acc\n",
        "_validation_loss = val_loss\n",
        "_validation_acc = val_acc"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "4LZpToji3Jxy",
        "outputId": "b29fcc87-7a7c-4630-89e2-0f6852144b41"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Loss graph\")\n",
        "plt.plot(_tranning_loss,label=\"Tranning Loss\")\n",
        "plt.plot(_validation_loss,label=\"Validation Loss\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "9I3Z4eo13K-T",
        "outputId": "1d7f9a0d-d05e-48a0-8147-629dd1b24931"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Accuracy graph\")\n",
        "plt.plot(_tranning_acc,label=\"Tranning Accuracy\")\n",
        "plt.plot(_validation_acc,label=\"Validation Accuracy\")\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xjZ8_bBAeMH"
      },
      "source": [
        "y_true_tensor = torch.tensor([])\n",
        "y_pred_tensor = torch.tensor([])\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "      for batch in valid_iter:\n",
        "          tweets, tweets_lengths = batch.tweets\n",
        "          predictions = model(tweets).squeeze(1)\n",
        "\n",
        "          rounded_preds = torch.round(torch.sigmoid(predictions))\n",
        "          correct = (rounded_preds == batch.sentiment).float() \n",
        "\n",
        "          # print(rounded_preds,batch.sentiment)\n",
        "\n",
        "          y_true_tensor = torch.cat((y_true_tensor,batch.sentiment))\n",
        "          y_pred_tensor = torch.cat((y_pred_tensor,rounded_preds))\n",
        "          # acc = binary_accuracy(predictions, batch.sentiment)\n",
        "          # print(acc)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgtBvybmnoi1"
      },
      "source": [
        "y_true = y_true_tensor.type(torch.LongTensor).tolist()\n",
        "y_pred = y_pred_tensor.type(torch.LongTensor).tolist()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlIt08Zenoeh",
        "outputId": "15de4226-c72b-4936-a946-ce681ccfea03"
      },
      "source": [
        "matrice = confusion_matrix(y_true,y_pred)\n",
        "matrice"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1584,   14],\n",
              "       [ 333,  128]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "oGkQQtFsnobs",
        "outputId": "d4142f81-f59d-4bff-82a2-8404dd16b261"
      },
      "source": [
        "df_cm = pd.DataFrame(matrice,columns=[0,1],index=[0,1])\n",
        "plt.figure(figsize=(5,5))\n",
        "sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 10},fmt='g',cmap='Blues',)\n",
        "plt.title(f\"Confusion matrix\")\n",
        "plt.show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x360 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMHEOvoLnoVG",
        "outputId": "fddad361-1d81-47bc-9554-76023db25ac3"
      },
      "source": [
        "classify_report = classification_report(y_true, y_pred, )\n",
        "print(classify_report)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.99      0.90      1598\n",
            "           1       0.90      0.28      0.42       461\n",
            "\n",
            "    accuracy                           0.83      2059\n",
            "   macro avg       0.86      0.63      0.66      2059\n",
            "weighted avg       0.84      0.83      0.79      2059\n",
            "\n"
          ]
        }
      ]
    }
  ]
}